{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Questão 2:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **GPT 4o**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pré-treinamento de Modelos de Linguagem de Grande Escala (LLMs) envolve várias etapas importantes. Aqui está um resumo do processo e a importância dos embeddings:\n",
    "\n",
    "### Processo de Pré-treinamento\n",
    "\n",
    "1. **Coleta de Dados:**\n",
    "   - Os modelos são treinados em grandes conjuntos de dados textuais, que podem incluir livros, artigos, páginas da web, entre outros.\n",
    "\n",
    "2. **Tokenização:**\n",
    "   - O texto é dividido em unidades menores chamadas tokens. Tokens podem ser palavras inteiras, subpalavras ou até caracteres, dependendo da abordagem de tokenização.\n",
    "\n",
    "3. **Arquitetura do Modelo:**\n",
    "   - Redes neurais profundas, como Transformers, são usadas devido à sua capacidade de capturar dependências contextuais em longas sequências de texto.\n",
    "\n",
    "4. **Tarefa de Treinamento:**\n",
    "   - Uma tarefa comum é o \"treinamento não supervisionado\" usando objetivos como **previsão de palavras mascaradas** (no caso de BERT) ou **modelagem de linguagem causal** (no caso de GPT).\n",
    "\n",
    "5. **Ajuste de Parâmetros:**\n",
    "   - Durante o treinamento, os parâmetros do modelo são ajustados para minimizar a diferença entre as previsões do modelo e as palavras reais em textos de exemplo.\n",
    "\n",
    "### Importância dos Embeddings\n",
    "\n",
    "- **Representação Vetorial:**\n",
    "  - Embeddings são representações vetoriais de palavras ou tokens. Eles capturam semelhanças semânticas, permitindo que palavras com significados semelhantes tenham representações próximas no espaço vetorial.\n",
    "\n",
    "- **Redução de Dimensionalidade:**\n",
    "  - Transformam dados categóricos de alta dimensão (como palavras) em uma forma contínua de menor dimensão, facilitando o processamento pelo modelo.\n",
    "\n",
    "- **Transferência de Conhecimento:**\n",
    "  - Embeddings pré-treinados podem ser usados em diferentes tarefas de processamento de linguagem natural, permitindo que o modelo aplique o que aprendeu em um contexto para outros.\n",
    "\n",
    "- **Contextualização:**\n",
    "  - Nos LLMs, embeddings podem ser contextuais, significando que a representação de uma palavra muda dependendo das palavras ao seu redor, capturando melhor o significado em diferentes contextos.\n",
    "\n",
    "Esses componentes são fundamentais para o sucesso dos LLMs na compreensão e geração de linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Llama-3.1-70B**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pré-treinamento de LLMs**\n",
    "==========================\n",
    "\n",
    "O pré-treinamento de LLMs (Large Language Models) é um processo fundamental para treinar modelos de linguagem que possam compreender e gerar texto de forma eficaz. O objetivo do pré-treinamento é fornecer ao modelo uma grande quantidade de texto não rotulado, permitindo que ele aprenda a representar a linguagem de forma geral.\n",
    "\n",
    "**Passos do pré-treinamento**\n",
    "---------------------------\n",
    "\n",
    "1. **Coleta de dados**: Uma grande quantidade de texto não rotulado é coletada de várias fontes, como livros, artigos, sites e outros recursos online.\n",
    "2. **Pré-processamento**: O texto coletado é pré-processado para remover caracteres especiais, pontuação e outros elementos que não sejam relevantes para o treinamento.\n",
    "3. **Tokenização**: O texto pré-processado é dividido em tokens, que são unidades básicas de linguagem, como palavras ou caracteres.\n",
    "4. **Treinamento**: O modelo de linguagem é treinado usando o conjunto de tokens, com o objetivo de prever o próximo token na sequência. Isso é feito usando uma técnica chamada \"masking\", onde alguns tokens são ocultos e o modelo precisa prever o token correto.\n",
    "5. **Otimização**: O modelo é otimizado usando algoritmos de otimização, como o Adam, para minimizar a perda entre a previsão do modelo e o token real.\n",
    "\n",
    "**Importância do conceito de embeddings em LLMs**\n",
    "---------------------------------------------\n",
    "\n",
    "Os embeddings são uma representação densa e compacta de palavras ou tokens em um espaço vetorial de alta dimensão. Eles são fundamentais para o funcionamento dos LLMs, pois permitem que o modelo capture a semântica e a sintaxe da linguagem de forma eficaz.\n",
    "\n",
    "**Características dos embeddings**\n",
    "---------------------------------\n",
    "\n",
    "* **Densidade**: Os embeddings são densos, o que significa que eles capturam a essência da palavra ou token em um espaço vetorial compacto.\n",
    "* **Semântica**: Os embeddings capturam a semântica da linguagem, permitindo que o modelo entenda o significado das palavras e tokens.\n",
    "* **Sintaxe**: Os embeddings também capturam a sintaxe da linguagem, permitindo que o modelo entenda a estrutura das frases e textos.\n",
    "\n",
    "**Vantagens dos embeddings**\n",
    "---------------------------\n",
    "\n",
    "* **Melhora a compreensão da linguagem**: Os embeddings permitem que o modelo compreenda a linguagem de forma mais profunda e eficaz.\n",
    "* **Aumenta a capacidade de generalização**: Os embeddings permitem que o modelo generalize melhor para novos textos e contextos.\n",
    "* **Reduz a dimensionalidade**: Os embeddings reduzem a dimensionalidade do espaço de entrada, o que pode ajudar a reduzir a complexidade do modelo e melhorar a eficiência computacional.\n",
    "\n",
    "Em resumo, o pré-treinamento de LLMs é um processo fundamental para treinar modelos de linguagem que possam compreender e gerar texto de forma eficaz. Os embeddings são uma representação densa e compacta de palavras ou tokens em um espaço vetorial de alta dimensão, e são fundamentais para o funcionamento dos LLMs. Eles permitem que o modelo capture a semântica e a sintaxe da linguagem de forma eficaz, e são essenciais para a compreensão da linguagem e a generalização para novos textos e contextos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gemini 1.5 Flash**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pré-treinamento de Large Language Models (LLMs) é um processo crucial que estabelece a base para suas capacidades impressionantes de processamento de linguagem natural.  Ele funciona essencialmente como uma fase de aprendizado inicial, onde o modelo aprende a estrutura e as regularidades da linguagem a partir de um vasto conjunto de dados de texto.  Esse processo difere do treinamento \"fino\" (fine-tuning), que adapta o modelo pré-treinado para tarefas específicas.\n",
    "\n",
    "Aqui está um detalhamento do processo:\n",
    "\n",
    "1. **Coleta de Dados:**  O primeiro passo envolve a coleta de um enorme corpus de texto. Isso pode incluir livros, artigos, código-fonte, páginas da web e muito mais. A escala desses dados é fundamental para o sucesso do pré-treinamento, com conjuntos de dados frequentemente contendo bilhões ou até trilhões de tokens (palavras ou sub-palavras).\n",
    "\n",
    "2. **Arquitetura do Modelo:**  Um modelo de linguagem, geralmente baseado em arquiteturas transformadoras como o Transformer, é selecionado.  Essas arquiteturas são projetadas para processar sequências de dados, como texto, de forma eficiente, capturando relações entre palavras e frases distantes na sequência.\n",
    "\n",
    "3. **Objetivo do Pré-treinamento:** O objetivo principal do pré-treinamento é ensinar o modelo a prever palavras faltantes em uma sequência de texto.  Existem várias abordagens para isso, sendo as mais comuns:\n",
    "\n",
    "    * **Predição de palavras mascaradas (Masked Language Modeling - MLM):**  Algumas palavras no texto de entrada são mascaradas (substituídas por um token especial), e o modelo é treinado para prever as palavras originais com base no contexto.  Este é o método usado no BERT, por exemplo.\n",
    "\n",
    "    * **Predição de próxima palavra (Next Sentence Prediction - NSP):**  O modelo é treinado para prever se duas sentenças são consecutivas em um texto.  Embora menos comum agora, ajudava o modelo a entender relações entre sentenças.\n",
    "\n",
    "    * **Predição de palavras aleatórias (Random Word Prediction):** O modelo tenta prever palavras aleatórias em uma sequência, focando em aprender representações de palavras mais robustas.\n",
    "\n",
    "\n",
    "4. **Treinamento:**  O modelo é treinado usando um algoritmo de otimização (como o Adam) para minimizar a diferença entre suas previsões e as palavras reais.  Este processo envolve iterações repetidas sobre o conjunto de dados, ajustando os pesos do modelo para melhorar sua precisão na previsão de palavras.  Essa etapa requer uma infraestrutura computacional significativa, devido ao tamanho dos modelos e dos dados.\n",
    "\n",
    "5. **Armazenamento do Modelo Pré-treinado:**  Após o treinamento, os pesos do modelo são armazenados.  Este modelo pré-treinado é então usado como ponto de partida para o treinamento fino em tarefas específicas.\n",
    "\n",
    "\n",
    "**A Importância dos Embeddings em LLMs:**\n",
    "\n",
    "Embeddings são representações vetoriais de palavras, frases ou até mesmo documentos inteiros.  Eles são cruciais para o funcionamento dos LLMs por várias razões:\n",
    "\n",
    "* **Captura de Semântica:**  Embeddings capturam a semântica das palavras, representando-as como pontos em um espaço vetorial de alta dimensional.  Palavras com significados semelhantes ficam próximas umas das outras nesse espaço, enquanto palavras com significados diferentes ficam mais distantes.  Isso permite que o modelo entenda as relações entre palavras e seu contexto.\n",
    "\n",
    "* **Generalização:**  Os embeddings aprendidos durante o pré-treinamento permitem que o modelo generalize para novos dados e tarefas.  Mesmo que o modelo não tenha visto uma palavra específica antes, ele pode inferir seu significado com base em sua proximidade com outras palavras no espaço vetorial.\n",
    "\n",
    "* **Eficiência Computacional:**  Representar palavras como vetores permite que o modelo processe a linguagem de forma mais eficiente do que métodos baseados em regras ou representações esparsas.\n",
    "\n",
    "* **Transferência de Aprendizado:**  Os embeddings aprendidos durante o pré-treinamento são transferidos para o treinamento fino, fornecendo um ponto de partida sólido para o aprendizado de tarefas específicas.  Isso permite que os modelos aprendam tarefas com conjuntos de dados menores, pois já possuem um conhecimento prévio da linguagem.\n",
    "\n",
    "\n",
    "Em resumo, o pré-treinamento é um processo fundamental para criar LLMs poderosos, e os embeddings são um componente essencial desse processo, permitindo que os modelos representem e processem a linguagem de forma eficiente e significativa.  A qualidade dos embeddings e a escala do pré-treinamento são fatores determinantes na performance final do modelo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Claude-3-Haiku**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O pré-treinamento de Modelos de Linguagem de Grande Porte (LLMs, do inglês Large Language Models) é um processo fundamental para a criação desses modelos poderosos. Ele envolve duas etapas principais:\n",
    "\n",
    "1. Coleta e processamento de dados de texto em larga escala:\n",
    "   - Os LLMs são treinados em grandes conjuntos de dados de texto, como livros, artigos, websites e outras fontes de linguagem natural.\n",
    "   - Esses dados passam por processos de limpeza, tokenização e outras técnicas de pré-processamento para preparar o texto para o treinamento.\n",
    "\n",
    "2. Treinamento do modelo de linguagem:\n",
    "   - O algoritmo de treinamento, geralmente baseado em redes neurais, é aplicado aos dados de texto preparados.\n",
    "   - O objetivo é ensinar o modelo a prever a próxima palavra em uma sequência de texto, com base no contexto fornecido.\n",
    "   - Durante o treinamento, o modelo aprende a capturar padrões, estruturas gramaticais, significados e outras características da linguagem.\n",
    "\n",
    "A importância do conceito de embeddings em LLMs:\n",
    "- Embeddings são representações vetoriais de palavras, frases ou outros elementos linguísticos.\n",
    "- No contexto dos LLMs, os embeddings desempenham um papel fundamental:\n",
    "  - Eles permitem que o modelo capture e represente o significado, a semântica e as relações entre as unidades linguísticas.\n",
    "  - Essa representação vetorial permite que o modelo realize tarefas como compreensão de linguagem natural, tradução, geração de texto e muito mais.\n",
    "  - Os embeddings são aprendidos durante o pré-treinamento e são posteriormente refinados e ajustados para tarefas específicas.\n",
    "  - Eles possibilitam que o modelo entenda e manipule a linguagem de maneira mais profunda e contextualizada, em vez de simplesmente lidar com sequências de palavras isoladas.\n",
    "\n",
    "Em resumo, o pré-treinamento é crucial para dotar os LLMs com um conhecimento linguístico amplo e profundo, enquanto os embeddings são a representação vetorial que permite que esses modelos compreendam e processem a linguagem de maneira sofisticada. Juntos, esses conceitos formam a base para a capacidade dos LLMs de realizarem uma ampla gama de tarefas envolvendo processamento de linguagem natural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ANÁLISE DAS RESPOSTAS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Llama-3.1-70B**: Esta resposta detalha o processo de pré-treinamento e a importância dos embeddings de forma clara e estruturada. Ele divide o pré-treinamento em etapas específicas e explora as características e vantagens dos embeddings, apresentando uma explicação fundamentada destacando a densidade, semântica e sintaxe.\n",
    "\n",
    "2. **GPT-4o**: Também segue uma estrutura razoável, detalhando o processo de pré-treinamento e discutindo os embeddings. A descrição inclui a funcionalidade dos embeddings no contexto dos LLMs, como a redução de dimensionalidade e a transferência de conhecimento, fornecendo uma visão abrangente e técnica.\n",
    "\n",
    "3. **Gemini 1.5 Flash**: O texto do Gemini 1.5 Flash é detalhado e explora tanto o pré-treinamento quanto a importância dos embeddings, com ênfase na aplicação prática dos embeddings, como a generalização e eficiência computacional. Ele também destaca o papel dos embeddings na transferência de aprendizado e na representação semântica.\n",
    "\n",
    "4. **Claude-3-Haiku**: Esta resposta enfoca mais a narrativa e menos nos detalhes técnicos, proporcionando uma visão geral mais acessível do pré-treinamento e dos embeddings. A explicação é mais generalista, mas ainda assim captura a essência do papel dos embeddings em LLMs, focando na capacidade de processamento de linguagem natural.\n",
    "\n",
    "Comparando as respostas, podemos ver que:\n",
    "- **Llama-3.1-70B** e **GPT-4o** fornecem respostas mais detalhadas e técnicas, adequadas para um público que deseja entender profundamente o mecanismo dos LLMs.\n",
    "- **Gemini 1.5 Flash** destaca a aplicação prática dos conceitos, sendo útil para quem está interessado nas implicações práticas dos embeddings e do pré-treinamento.\n",
    "- **Claude-3-Haiku** oferece uma visão mais simplificada, ideal para leitores que buscam uma compreensão básica sem entrar nos aspectos técnicos detalhados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As respostas fornecidas pelos modelos são todas corretas essencialmente, porém diferem em profundidade e precisão técnica. Em termos de erros/imprecisões e precisão técnica, pode-se notar:\n",
    "\n",
    "1. **Llama-3.1-70B**:\n",
    "   - **Precisão**: Alta. A resposta detalha cada etapa do processo de pré-treinamento e descreve com precisão o papel dos embeddings em LLMs.\n",
    "   - **Erros/Imprecisões**: Não apresenta erros claros, mas poderia expandir sobre como os embeddings são atualizados durante o treinamento para capturar melhor a semântica e a sintaxe.\n",
    "\n",
    "2. **GPT-4o**:\n",
    "   - **Precisão**: Alta. Similar ao Llama, oferece uma descrição detalhada e técnica que é precisa e abrangente.\n",
    "   - **Erros/Imprecisões**: Não há erros significativos, mas poderia incluir informações sobre diferentes tipos de arquiteturas de modelos, além dos Transformers, para dar uma visão mais completa.\n",
    "\n",
    "3. **Gemini 1.5 Flash**:\n",
    "   - **Precisão**: Muito alta. Esta resposta vai além das outras ao detalhar diferentes métodos de treinamento, como MLM e NSP, e discute amplamente o impacto dos embeddings.\n",
    "   - **Erros/Imprecisões**: Enquanto a resposta é rica em detalhes, ela poderia esclarecer que o NSP não é mais tão comumente usado em modelos mais novos como o BERT.\n",
    "\n",
    "4. **Claude-3-Haiku**:\n",
    "   - **Precisão**: Moderada. Fornece uma visão geral acessível, mas é menos técnica e menos detalhada do que as outras.\n",
    "   - **Erros/Imprecisões**: A resposta poderia ser vista como superficial para alguém que precisa de uma compreensão técnica profunda. Não menciona especificidades dos diferentes tipos de embeddings (como estáticos vs. contextuais).\n",
    "\n",
    "Considerando a precisão e completude, a resposta do **Gemini 1.5 Flash** se destaca por sua abordagem detalhada e pela inclusão de exemplos específicos de técnicas de pré-treinamento, tornando-a particularmente rica em termos de informação. Esta resposta seria útil para um público que já possui algum entendimento de LLMs e deseja uma explicação mais profunda sobre o funcionamento interno e técnicas aplicadas no pré-treinamento de modelos de linguagem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Questão 3:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiências e suas representações dimensionais:\n",
    "\n",
    "### 1. viagem memorável ao Japão\n",
    "- **Cor (7/10)**: Colorido, refletindo as luzes brilhantes de Tóquio e as flores de cerejeira em Kyoto.\n",
    "- **Emoção (9/10)**: Fascínio e alegria, devido à descoberta de uma cultura rica e excitante.\n",
    "- **Complexidade (8/10)**: Alta, devido à diversidade de locais visitados e à barreira do idioma.\n",
    "- **Abstração (4/10)**: Moderada, com muitos elementos concretos como templos e jardins, mas também experiências subjetivas e sensoriais.\n",
    "- **Interatividade (7/10)**: Bastante interativa, envolvendo muita comunicação com os locais e participação em atividades culturais.\n",
    "\n",
    "### 2. Jantar de aniversário com prato favorito: Lasanha\n",
    "- **Cor (6/10)**: Vibrante, com o vermelho do molho de tomate e o amarelo do queijo.\n",
    "- **Emoção (8/10)**: Felicidade e conforto, pois é uma refeição que traz lembranças de casa.\n",
    "- **Complexidade (3/10)**: Simples, focada em saborear um prato conhecido e adorado.\n",
    "- **Abstração (2/10)**: Baixa, é uma experiência muito tangível e direta.\n",
    "- **Interatividade (3/10)**: Relativamente baixa, focada mais na comida do que na interação.\n",
    "\n",
    "### 3. Primeira apresentação em uma conferência importante\n",
    "- **Cor (5/10)**: Neutra, dominada pelo ambiente formal do evento.\n",
    "- **Emoção (10/10)**: Nervosismo e excitação, um marco significativo na carreira.\n",
    "- **Complexidade (9/10)**: Alta, envolvendo preparação intensa e domínio do conteúdo.\n",
    "- **Abstração (6/10)**: Alguma abstração envolvida na tradução de ideias complexas para uma apresentação compreensível.\n",
    "- **Interatividade (8/10)**: Alta, com muita interação com o público durante e após a apresentação.\n",
    "\n",
    "### 4. Assistir ao nascer do sol no topo de uma montanha\n",
    "- **Cor (8/10)**: Intensamente colorido, com tons vibrantes de laranja, rosa e roxo.\n",
    "- **Emoção (7/10)**: Paz e admiração pela beleza natural.\n",
    "- **Complexidade (4/10)**: Moderadamente complexo, principalmente devido ao esforço físico para chegar ao topo.\n",
    "- **Abstração (7/10)**: Bastante abstrato, uma experiência que eleva o espírito e provoca reflexão.\n",
    "- **Interatividade (2/10)**: Baixa, uma experiência mais contemplativa e pessoal.\n",
    "\n",
    "### 5. Primeiro encontro com uma pessoa especial\n",
    "- **Cor (6/10)**: Suave, com um ambiente aconchegante de um café tranquilo.\n",
    "- **Emoção (9/10)**: Emoção intensa, mistura de ansiedade e esperança.\n",
    "- **Complexidade (6/10)**: Moderadamente complexa, com nervosismo e esforço para causar uma boa impressão.\n",
    "- **Abstração (5/10)**: Um misto de momentos concretos e sentimentos abstratos.\n",
    "- **Interatividade (10/10)**: Extremamente interativa, focada na comunicação e no conhecimento mútuo.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Questão 4:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analisando os vetores de cada experiência e identificando semelhanças com base em seus valores em cada dimensão:\n",
    "\n",
    "### Vetores das Experiências\n",
    "1. **Viagem ao Japão**: [7, 9, 8, 4, 7]\n",
    "2. **Jantar de aniversário (lasanha)**: [6, 8, 3, 2, 3]\n",
    "3. **Primeira apresentação em conferência**: [5, 10, 9, 6, 8]\n",
    "4. **Nascer do sol no topo da montanha**: [8, 7, 4, 7, 2]\n",
    "5. **Primeiro encontro**: [6, 9, 6, 5, 10]\n",
    "\n",
    "### Comparação\n",
    "Comparando os vetores ponto a ponto e observando valores próximos em dimensões semelhantes:\n",
    "\n",
    "- **Viagem ao Japão** e **Primeiro encontro**: \n",
    "  - Cor: 7 vs 6\n",
    "  - Emoção: 9 vs 9\n",
    "  - Complexidade: 8 vs 6\n",
    "  - Abstração: 4 vs 5\n",
    "  - Interatividade: 7 vs 10\n",
    "  - **Justificativa**: Ambas as experiências têm valores altos em emoção e cor relativamente alta, indicando uma experiência colorida e emocionalmente intensa. Ambas têm níveis moderados a altos de interatividade e complexidade.\n",
    "\n",
    "- **Primeira apresentação em conferência** e **Primeiro encontro**:\n",
    "  - Cor: 5 vs 6\n",
    "  - Emoção: 10 vs 9\n",
    "  - Complexidade: 9 vs 6\n",
    "  - Abstração: 6 vs 5\n",
    "  - Interatividade: 8 vs 10\n",
    "  - **Justificativa**: Estas experiências compartilham altos níveis de emoção e interatividade. As dimensões de emoção e interatividade são bastante elevadas, o que sugere que ambas são situações que requerem engajamento ativo e são carregadas de expectativas e ansiedade.\n",
    "\n",
    "### Conclusão\n",
    "O **Primeiro encontro** e a **Primeira apresentação em conferência** são os mais similares em termos de vetores. Ambas são situações de alto impacto emocional e interativo, onde a comunicação e a apresentação pessoal desempenham um papel crucial. A semelhança aqui é baseada no alto nível de emoção e interatividade, e em níveis moderadamente altos de complexidade e abstração, sugerindo situações que envolvem significativa interação pessoal e estresse emocional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Questão 5:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vetores das Experiências\n",
    "\n",
    "1. **Viagem ao Japão**: [7, 9, 8, 4, 7]\n",
    "2. **Jantar de aniversário (lasanha)**: [6, 8, 3, 2, 3]\n",
    "3. **Primeira apresentação em conferência**: [5, 10, 9, 6, 8]\n",
    "4. **Nascer do sol no topo da montanha**: [8, 7, 4, 7, 2]\n",
    "5. **Primeiro encontro**: [6, 9, 6, 5, 10]\n",
    "\n",
    "### Obras de Arte\n",
    "\n",
    "- **\"A Noite Estrelada\"**: [9, 8, 7, 6, 3]\n",
    "- **\"Guernica\"**: [2, 10, 9, 8, 5]\n",
    "- **\"A Persistência da Memória\"**: [6, 7, 6, 9, 4]\n",
    "- **\"O Grito\"**: [7, 10, 6, 7, 4]\n",
    "- **\"A Criação de Adão\"**: [8, 7, 8, 3, 6]\n",
    "\n",
    "\n",
    "### **A)** Comparação por Emoção\n",
    "\n",
    "1. **Viagem ao Japão (9)** - \"O Grito\" (10) ou \"Guernica\" (10)\n",
    "2. **Jantar de aniversário (8)** - \"A Noite Estrelada\" (8)\n",
    "3. **Primeira apresentação em conferência (10)** - \"O Grito\" (10) ou \"Guernica\" (10)\n",
    "4. **Nascer do sol no topo da montanha (7)** - \"A Persistência da Memória\" (7)\n",
    "5. **Primeiro encontro (9)** - \"O Grito\" (10) ou \"Guernica\" (10)\n",
    "\n",
    "### **B)** Média de Complexidade Visual das Experiências\n",
    "\n",
    "- Média = (8 + 3 + 9 + 4 + 6) / 5 = 6\n",
    "\n",
    "### Obra Mais Próxima da Média de Complexidade (6)\n",
    "\n",
    "- **\"A Noite Estrelada\"** (7)\n",
    "- **\"Guernica\"** (9)\n",
    "- **\"A Persistência da Memória\"** (6) - Exatamente 6\n",
    "- **\"O Grito\"** (6) - Exatamente 6\n",
    "- **\"A Criação de Adão\"** (8)\n",
    "\n",
    "### Conclusões\n",
    "- **Em termos de emoção**, \"O Grito\" e \"Guernica\" são as que mais se alinham com as experiências mais intensas emocionalmente, enquanto \"A Noite Estrelada\" se alinha perfeitamente com o jantar de aniversário.\n",
    "- **Em termos de complexidade visual**, tanto \"A Persistência da Memória\" quanto \"O Grito\" se alinham exatamente com a média das experiências, destacando-se como as obras que melhor representam a complexidade visual média das experiências pessoais descritas."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
